{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#                 *** Imports & General ***\n",
    "# =============================================================================\n",
    "from __future__ import division\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import binascii\n",
    "from bisect import bisect_right\n",
    "from heapq import heappop, heappush\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag as pos_tagger\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "from nltk.corpus import stopwords as stopwords_nltk, wordnet\n",
    "\n",
    "import glob\n",
    "import time\n",
    "import codecs\n",
    "import binascii\n",
    "\n",
    "import string\n",
    "import sys\n",
    "import json\n",
    "import itertools\n",
    "from distutils.util import strtobool\n",
    "\n",
    "srcdir = \"corpus\"\n",
    "#srcdir = \"source2\"\n",
    "outdir = \"Output/\"\n",
    "    \n",
    "src_dir_files = [f for f in glob.glob(srcdir + \"/\" + \"*.txt\")]\n",
    "\n",
    "    \n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/costakis/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!python -c 'import nltk; nltk.download(\"wordnet\")'\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just started reading each file and k-shingling...\n",
      "Finished reading: 100/902\n",
      "Finished reading: 200/902\n",
      "Finished reading: 300/902\n",
      "Finished reading: 400/902\n",
      "Finished reading: 500/902\n",
      "Finished reading: 600/902\n",
      "Finished reading: 700/902\n",
      "Finished reading: 800/902\n",
      "Finished reading: 900/902\n",
      "\n",
      "\n",
      "READY. Total time needed for reading, preprocessing, and shingling 902 documents: 262.36 seconds.\n"
     ]
    }
   ],
   "source": [
    "#################################################################################\n",
    "#                 *** Reading - Preprocessing - Shingling ***\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "#            *** Preprocessing Text ***\n",
    "# -------------------------------------------------------------------------\n",
    "# Preprocessing the text of a document \n",
    "# Lower case of all words, stemming, removing symbols and numbers\n",
    "# -------------------------------------------------------------------------\n",
    "def preprocess_text(fid):\n",
    "    words = [word.lower() for line in fid for word in line.split()] \n",
    "    \n",
    "    SYMBOLS = '\\'\\\"_{}()[].,:;+-*/&|<>=~$1234567890'\n",
    "    clear_words = [item.translate(str.maketrans('','',SYMBOLS)).strip() for item in words]\n",
    "    clear_words = filter(None, clear_words)\n",
    "\n",
    "    lemmatized_words = []\n",
    "    for w in clear_words:\n",
    "        lemmatized_words.append(lemmatizer.lemmatize(w))\n",
    "        \n",
    "    \n",
    "    return(lemmatized_words)\n",
    "# -------------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "\n",
    "# =============================================================================\n",
    "#       *** Converting documents to sets of Shingles ***\n",
    "# -----------------------------------------------------------------------------\n",
    "# In this part we are converting documents to sets of shingles.\n",
    "# We are iterating through each file of the corpus reading it.\n",
    "# We are preprocessing their text by stemmening and removing symbols and then\n",
    "# we are creating k-shingles with these words.\n",
    "# We are keeping them in sets() which means that there are no dublicates!\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "print(\"Just started reading each file and k-shingling...\")\n",
    "\n",
    "\n",
    "# We use this variable in order to keep track of the Shingles.\n",
    "# Whenever a new shingle is added, we increment this by one.\n",
    "#curShingleID = 0\n",
    "\n",
    "# In this list we are going to keep the names of all document of the corpus.\n",
    "docNames_list = []\n",
    "\n",
    "#docsAsShingleSets = {};\n",
    "  \n",
    "\n",
    "# This keeps the total number of documets inside the corpus.\n",
    "numDocs = 0  \n",
    "# Creating a dictionary which keeps all the Shingle sets of each document of the corpus.\n",
    "# The key of a document in this dictionary is its id.\n",
    "corpusShingles = {}\n",
    "\n",
    "# Keeps the number of documents we have read.\n",
    "# We will use this just for printing information during the process.\n",
    "counter = 0;\n",
    "# For each document inside the corpus\n",
    "for src_file in src_dir_files:  \n",
    "\n",
    "    counter += 1 \n",
    "    \n",
    "    # Open the document\n",
    "    fid = open(src_file, newline='')\n",
    "    \n",
    "    # Preprocess document's text (stemming, removing symbols & numbers, etc.)\n",
    "    words = preprocess_text(fid)\n",
    "    \n",
    "    numDocs += 1\n",
    "  \n",
    "    # Keeping the name of the document  \n",
    "    docName = src_file.split(\"/\")[1]\n",
    "  \n",
    "    # Adding the name of the document in the list of all document names of the corpus  \n",
    "    docNames_list.append(docName)\n",
    "    \n",
    "    \n",
    "    # This is a set which contains all k-Shingles created from the current document.\n",
    "    # In fact, each item of the set contains a hash value created with k-Shingles.\n",
    "    # There are no dublicates of k-Shingles since we have a set\n",
    "    docShingles = set()\n",
    "\n",
    "  \n",
    "    # For each word of the text\n",
    "    for indWord in range(0, len(words) - 2):\n",
    "        \n",
    "        # Creating a Shingle by taking words in a raw from the text.\n",
    "        shingle = words[indWord] + \" \" + words[indWord + 1] + \" \" + words[indWord+ 2] \n",
    "\n",
    "        \n",
    "        # Now we need to compress the Shingle.\n",
    "        # To compress the Shingle, we hash it to 4 bytes (32 bits).\n",
    "        hashedShingle = binascii.crc32(str.encode(shingle)) & 0xffffffff\n",
    "    \n",
    "        # Adding the hashed Shingle in the set of k-Shingles of the document.\n",
    "        # If this specific Shingle already exists, then it won't get inserted again.\n",
    "        docShingles.add(hashedShingle)\n",
    "  \n",
    "    # Storing the set of Shingles of this document to the dictionary of Shingles of the whole corpus.\n",
    "    corpusShingles[docName] =  docShingles\n",
    "    \n",
    "    # Closing file\n",
    "    fid.close()  \n",
    "    \n",
    "    if counter % 100 == 0:\n",
    "        print(\"Finished reading: \" + str(counter) + \"/902\")\n",
    "        \n",
    " \n",
    "    # ===========================================================================\n",
    "# Print total time need for reading, preprocessing and shingling all corpus documents.\n",
    "print(\"\\n\\nREADY. Total time needed for reading, preprocessing, and shingling \" +  str(numDocs) + str(\" documents: %.2f seconds.\") % (time.time() - t0))\n",
    "\n",
    " \n",
    "#print '\\nAverage shingles per doc: %.2f' % (totalShingles / numDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just started creating signatures...\n",
      "Finished creating signatures of: 100/902\n",
      "Finished creating signatures of: 200/902\n",
      "Finished creating signatures of: 300/902\n",
      "Finished creating signatures of: 400/902\n",
      "Finished creating signatures of: 500/902\n",
      "Finished creating signatures of: 600/902\n",
      "Finished creating signatures of: 700/902\n",
      "Finished creating signatures of: 800/902\n",
      "Finished creating signatures of: 900/902\n",
      "\n",
      "\n",
      "READY. Total time needed for MinHash signatures: 1406.46sec\n"
     ]
    }
   ],
   "source": [
    "#################################################################################\n",
    "#                 *** Creating MinHash Signatures ***\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "#       *** Creating random variables for the hashing functions ***\n",
    "# -------------------------------------------------------------------------\n",
    "def createVariables(numHashes):\n",
    "    \n",
    "    maxShingleID = 2**32-1 # 2^32-1 is the max hashed Shingle id we can have\n",
    "\n",
    "    # List which contains all random variables thar are going to be created.\n",
    "    variables = []\n",
    "  \n",
    "    # Create a random variable for each one of the hash functions\n",
    "    for i in range(numHashes):\n",
    "        randomVariable = random.randint(0, maxShingleID) \n",
    "        \n",
    "        # Variable created must be unique.\n",
    "        while randomVariable in variables: \n",
    "            randomVariable = random.randint(0, maxShingleID) \n",
    "    \n",
    "        # Appending new variable to the list of random variables.\n",
    "        variables.append(randomVariable)\n",
    "    \n",
    "    return variables\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "#            *** Creating random hashing functions ***\n",
    "# -------------------------------------------------------------------------\n",
    "def randHashFunc_gen(hashnum):\n",
    "   \n",
    "    # Keeping the number of hash functions.\n",
    "    numHashes = hashnum;\n",
    "    \n",
    "    # Create a and b variables for hash functions.\n",
    "    As= createVariables(numHashes)\n",
    "    Bs = createVariables(numHashes)\n",
    "    \n",
    "    signatureMatrix = MinHashSig_gen(numHashes, As, Bs)\n",
    "    return signatureMatrix\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#       *** Creating random hash functions ***\n",
    "# -----------------------------------------------------------------------------\n",
    "# The hash function we are going to create follo this rule:\n",
    "#              h(x) = (a*x + b) mod c\n",
    "# Where a,b are random variables and c is a prime number greater than \n",
    "# the maximun value of a hashed Shingle (shingleID)\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def MinHashSig_gen(numHashes, As, Bs):\n",
    "\n",
    "    print(\"Just started creating signatures...\")\n",
    "\n",
    "    \n",
    "    # This is a list of lists or lists of signatures.\n",
    "    # Each row represents a document from the corpus and\n",
    "    # each column represents the minhash value.\n",
    "    signatureMatrix = []\n",
    "\n",
    "   \n",
    "    #-------------------------------------------- \n",
    "    # Now it is the time for minshashing...\n",
    "    # For each of the corpus' documents we are going to hash all its compressed k-Shingles\n",
    "    # and finally keep the lowest value.\n",
    "    #-------------------------------------------- \n",
    "    \n",
    "    # This is the largest prime number after the maximun id of Shingle \n",
    "    # that we are going to assign.\n",
    "    nextLargestPrime = 4294967311\n",
    "    \n",
    "    \n",
    "    # Keeps the number of documents we have read.\n",
    "    # We will use this just for printing information during the process.\n",
    "    counter = 0;\n",
    "    \n",
    "    # For each corpus' document\n",
    "    for docName in docNames_list:\n",
    "\n",
    "        counter += 1\n",
    "        \n",
    "        # This is gonna keep the signature we are creating for the doc.\n",
    "        signature = []\n",
    "        \n",
    "        # Getting the set with all Shingles of the document.\n",
    "        setShingle = corpusShingles[docName]\n",
    "\n",
    "        # For each one of the random hash functions\n",
    "        for i in range(0, numHashes):\n",
    "            minHashValue = float(\"inf\")\n",
    "            # For each shingle in the document\n",
    "            for shingleID in setShingle:\n",
    "                \n",
    "                # Creating hash function ( h(x) = (a*x + b) mod c )\n",
    "                hashFuncValue = (As[i] * shingleID + Bs[i]) % nextLargestPrime\n",
    "                # Keeping the lowest value\n",
    "                if hashFuncValue < minHashValue:\n",
    "                    minHashValue = hashFuncValue\n",
    "\n",
    "            # Add the smallest hash code value as component number 'i' of the signature.\n",
    "            \n",
    "            # Appending the min hashing value in document's signature.\n",
    "            signature.append(minHashValue)\n",
    "\n",
    "        # Store the MinHash signature for this document.\n",
    "        signatureMatrix.append(signature)\n",
    "        \n",
    "        # Prints information about the progress\n",
    "        if counter % 100 == 0:\n",
    "            print(\"Finished creating signatures of: \" + str(counter) + \"/\" + str(numDocs))\n",
    "    \n",
    "    return signatureMatrix\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Time this step.\n",
    "t0 = time.time()\n",
    "\n",
    "signatureMatrix = randHashFunc_gen(150)\n",
    "\n",
    "# Calculate the elapsed time (in seconds)\n",
    "elapsed = (time.time() - t0)\n",
    "        \n",
    "print( \"\\n\\nREADY. Total time needed for MinHash signatures: %.2fsec\" % elapsed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "READY. Total time needed for LSH: 19.68sec\n"
     ]
    }
   ],
   "source": [
    "#################################################################################\n",
    "#                 *** LSH ***\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "#            *** Creating a list of buckets for all bands ***\n",
    "# -------------------------------------------------------------------------\n",
    "# Creating a list of lists. \n",
    "# Each item of the list is like a pointer to the bucket of each band.\n",
    "# The list of each one of these items, represents the buckets.\n",
    "# -------------------------------------------------------------------------\n",
    "def create_listOfBuckets(bands, bucketLength):\n",
    "    listOfAllBuckets = []\n",
    "    for aBand in range(bands):\n",
    "        #for index in range(bucketLength):\n",
    "           # listOfAllBuckets.append([])\n",
    "        listOfAllBuckets.append([[] for i in range(bucketLength)])\n",
    "    return listOfAllBuckets\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "#            *** Creating random hashing functions ***\n",
    "# -------------------------------------------------------------------------\n",
    "def jaccard_similarity(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union\n",
    "\n",
    "# =============================================================================\n",
    "#       *** Applying LSH  ***\n",
    "# -----------------------------------------------------------------------------\n",
    "# Separate Signatures Matrix in bands and rows. Each band can have multiple rows.\n",
    "# It must be: Number of Hash Functions = bands * rows\n",
    "# The idea here is that we are putting bands of documents in buckets by hashing.\n",
    "# If two documents for any of their bands (which represent the same rows)\n",
    "# fall in the same bucket, then we consider them as csnditares for being plagiarised.\n",
    "# In this case we calculate their Jaccard similarity. If this is greater than\n",
    "# a threshold we have already defined, then we finally consider them plagiarised.\n",
    "# =============================================================================\n",
    "\n",
    "def LSH_func(signatureMatrix,t, b, r):\n",
    "   \n",
    "    \n",
    "    bands = b\n",
    "    rows = r\n",
    "    \n",
    "    # It must always be: Number of Hash Functions = bands * rows.\n",
    "    if bands * rows != len(signatureMatrix[0]):\n",
    "        raise \"ERROR: Bands * rows are not equal to number of hash function.\"\n",
    "   \n",
    "    # Creating a bucket list for all bands.\n",
    "    listOfAllBuckets = create_listOfBuckets(bands,211)\n",
    "\n",
    "    # This is a dictionary which contains all possible pairs of plagiarised documents\n",
    "    # and their Jaccard Similarity.\n",
    "    candidates = {}\n",
    "    \n",
    "    i = 0\n",
    "    # For each of the bands\n",
    "    for aBand in range(bands):\n",
    "        # Taking the buckets of this specific band\n",
    "        buckets = listOfAllBuckets[aBand]        \n",
    "        band = [ele[i:i+rows] for ele in signatureMatrix]\n",
    "        \n",
    "        # For each of the partial signatures of this specific band,\n",
    "        # hash it, in a bucket.\n",
    "        for row in range(len(band)):\n",
    "                     \n",
    "            key = int(sum(band[row][:]) % len(buckets))\n",
    "            buckets[key].append(row)\n",
    "        i = i + rows\n",
    "\n",
    "        # For each part of the bucket\n",
    "        for item in buckets:\n",
    "            \n",
    "            # If there are more than one hashed in the shame bucket\n",
    "            if len(item) > 1:\n",
    "                pairNames = (docNames_list[item[0]], docNames_list[item[1]])\n",
    "                pair = (item[0], item[1])\n",
    "                \n",
    "                # If the pair of possible plagiarised documents\n",
    "                # ther is not already in the list of all candidates\n",
    "                if pair not in candidates:\n",
    "                    \n",
    "                    # We are calculating their Jaccard Similarity\n",
    "                    #doc1 = corpusShingles[docNames_list[item[0]]]\n",
    "                    #doc2 = corpusShingles[docNames_list[item[1]]]\n",
    "                    \n",
    "                    \n",
    "                    #Jsim = (len(doc1.intersection(doc2)) / len(doc1.union(doc2)))\n",
    "                    \n",
    "                    Jsim = jaccard_similarity(corpusShingles[docNames_list[item[0]]],corpusShingles[docNames_list[item[1]]])\n",
    "                    \n",
    "                    # If the Jaccard Similarity of the candidate pair is greater than\n",
    "                    # the defined threshold, we add them in the dictionary of found plagiarised\n",
    "                    # documents.\n",
    "                    if Jsim >= t:\n",
    "                        candidates[pairNames] = Jsim\n",
    "                        \n",
    "\n",
    "\n",
    "   \n",
    "    sortedCandidates = sorted(candidates.items(),key=itemgetter(1), reverse=True)\n",
    "\n",
    "    return candidates,sortedCandidates\n",
    "# =============================================================================\n",
    "\n",
    "# Time this step.\n",
    "t0 = time.time()\n",
    "candidates,sortedCandidates = LSH_func(signatureMatrix,0.001, 15,10)\n",
    "\n",
    "\n",
    "# Calculating elapsed time\n",
    "elapsed = (time.time() - t0)\n",
    "print(\"\\n\\nREADY. Total time needed for LSH: %.2fsec\" % elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of plagiarised documents in the ground_truth.tsv file is: 451\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "======================================================================================\n",
      "These are all similar pairs found and their Jaccard Similarity.\n",
      "\n",
      "======================================================================================\n",
      "         Document_1     Document_2  Jaccard_Sim\n",
      "0     document05549  document05998     0.031695\n",
      "1     document04042  document03507     0.029815\n",
      "2     document04735  document05663     0.028590\n",
      "3     document03604  document07095     0.028317\n",
      "4     document03927  document06493     0.027762\n",
      "...             ...            ...          ...\n",
      "1718  document00557  document07181     0.001007\n",
      "1719  document00630  document04219     0.001004\n",
      "1720  document03612  document03331     0.001002\n",
      "1721  document00631  document00343     0.001001\n",
      "1722  document05789  document03412     0.001001\n",
      "\n",
      "[1723 rows x 3 columns]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The total number of plagiarised documents is: 451.0\n",
      "Total number of plagiarized documents found is: 725\n",
      "The correct guesses are: 369/725\n",
      "The correct guesses out of all plagiarised documents are: 369/451.0\n",
      "\n",
      "False Positives: 356\n",
      "False Negatives: 82.0\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "#                 *** STATISTICS ***\n",
    "#################################################################################\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def get_Statistics():\n",
    "\n",
    "    # Creating a panda dataframe for the file with the labels of documents\n",
    "    df = pd.read_csv(\"ground_truth.tsv\", sep='\\t', usecols = ['doc_name','plagiarism'])\n",
    "    total = df['plagiarism'].sum()\n",
    "    print (\"The total number of plagiarised documents in the ground_truth.tsv file is: \" + str(total))\n",
    "\n",
    "\n",
    "\n",
    "    # reading all files from corpus and keeping their names\n",
    "    data = []\n",
    "    for file in sorted(os.listdir(srcdir)):\n",
    "        data.append(file)\n",
    "    df_file = pd.DataFrame(data, columns=['File'])\n",
    "    df_file = df_file[~df_file.File.str.contains(\".xml\")]\n",
    "    df_file['File'] = df_file['File'].str.replace('source-', '')\n",
    "    df_file['File'] = df_file['File'].str.replace('.txt', '')\n",
    "\n",
    "    #--------------------------------------------------------------------------------------\n",
    "    # Now, we want to keep the values 0 or 1 for only the documents which \n",
    "    # are included in our corpus.\n",
    "    # So we are merging existing files in the corpus and all files we know their plagiarism\n",
    "    # in order to keep the conjuction of them.\n",
    "    #--------------------------------------------------------------------------------------\n",
    "    df_merge=df.merge(df_file, left_on='doc_name', right_on='File', how='outer')\n",
    "\n",
    "    # If a row has a NaN value, means that the file included in this row\n",
    "    # does not belong to the corpus files, so we are removing it.\n",
    "    df_merge = df_merge.dropna()\n",
    "\n",
    "\n",
    "    #print(sort)\n",
    "\n",
    "\n",
    "\n",
    "    #>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    # Processing plagiarised pairs found\n",
    "    #>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "\n",
    "    # Creating a panda dataframe from all the plagiarized documents \n",
    "    # and their similarities.\n",
    "    df_similars = pd.DataFrame.from_records(sortedCandidates)\n",
    "    df_similars.columns = ['Pair', 'Jaccard_Sim']\n",
    "\n",
    "    # Giving names to columns\n",
    "    df_similars['Document_1'], df_similars['Document_2'] = df_similars.Pair.str\n",
    "\n",
    "    # Deleting first column\n",
    "    del df_similars['Pair']\n",
    "\n",
    "    # Changing the order of columns\n",
    "    df_similars = df_similars[['Document_1', 'Document_2','Jaccard_Sim']]\n",
    "    df_allSimilars = df_similars\n",
    "\n",
    "    # Removing .txt and source- from the file names\n",
    "    df_similars['Document_1'] = df_similars['Document_1'].str.replace('source-', '')\n",
    "    df_similars['Document_2'] = df_similars['Document_2'].str.replace('source-', '')\n",
    "    df_similars['Document_1'] = df_similars['Document_1'].str.replace('.txt', '')\n",
    "    df_similars['Document_2'] = df_similars['Document_2'].str.replace('.txt', '')\n",
    "    print(\"\\n\\n\\n\")\n",
    "    print(\"======================================================================================\")\n",
    "    print(\"These are all similar pairs found and their Jaccard Similarity.\\n\")\n",
    "    print(\"======================================================================================\")\n",
    "    print(df_similars) \n",
    "    print(\"\\n\\n\\n\")\n",
    "\n",
    "\n",
    "    #>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    # Merging all columns\n",
    "    # By doing this we will be able to find out very easily if a document which \n",
    "    # was found as plagiarised, is indeed plagiarized or not.\n",
    "    #>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    #df_allPlags = pd.DataFrame(columns= ['Documents'])\n",
    "\n",
    "\n",
    "    # Creating a list of all rows of the dataframe which contains\n",
    "    # all the similar documents found.\n",
    "    similars_set = set()  \n",
    "    # Iterate over each row \n",
    "    for index, rows in df_similars.iterrows(): \n",
    "        # Create list for the current row \n",
    "        doc1 =rows.Document_1\n",
    "        similars_set.add(doc1)\n",
    "\n",
    "        doc2 = rows.Document_2\n",
    "        similars_set.add(doc2)\n",
    "\n",
    "    #print(len(similars_set))\n",
    "\n",
    "\n",
    "    # Converting set of plagiarized documents to dataframe\n",
    "    df_allPlags = pd.DataFrame(list(similars_set), columns= ['Documents'])\n",
    "\n",
    "\n",
    "    # Merging\n",
    "    df_allPlags=df.merge(df_allPlags, left_on='doc_name', right_on='Documents', how='outer')\n",
    "    df_allPlags = df_allPlags.dropna()\n",
    "    #print(\"======================================================================================\")\n",
    "    #print(\"Check if plagiarized documents are indeed plagiarized according to ground_truth.csv\\n\")\n",
    "    #print(\"======================================================================================\")\n",
    "    #print(df_allPlags.to_string())\n",
    "\n",
    "\n",
    "    print(\"\\n\\n\\n\")\n",
    "\n",
    "    # Printing the total number of plagiarized documents in our corpus\n",
    "    # according to ground_truth file.\n",
    "    total = df_merge['plagiarism'].sum()\n",
    "    print (\"The total number of plagiarised documents is: \" + str(total))\n",
    "\n",
    "    # Printing the total number of plagiarised documents we found.\n",
    "    print(\"Total number of plagiarized documents found is: \" + str(len(df_allPlags)))\n",
    "    \n",
    "\n",
    "\n",
    "    #-------------------------------------------------------\n",
    "    # Here we are going to calculate how many of the real \n",
    "    # plagiarized documents we found\n",
    "    #-------------------------------------------------------\n",
    "\n",
    "    #df_allPlags is data frame which contains all the \n",
    "    # plagiarised documents found and the value 0 or 1\n",
    "    # which says the truth. So we are just goind to sum\n",
    "    # all the correct guesses.\n",
    "\n",
    "    correct = df_allPlags.loc[df['plagiarism'] == 1, 'plagiarism'].sum()\n",
    "\n",
    "    # Printing the total number of correct guesses out of the whole number of guesses\n",
    "    print(\"The correct guesses are: \" + str(correct) + \"/\" + str(len(df_allPlags)))\n",
    "\n",
    "    # Printing the total number of correct guesses out of the real number of plagiarised documets\n",
    "    print(\"The correct guesses out of all plagiarised documents are: \" + str(correct) + \"/\" + str(total))\n",
    "    print(\"\\nFalse Positives: \" + str((len(df_allPlags) - correct)))\n",
    "    print(\"False Negatives: \" + str(total - correct))\n",
    "    \n",
    "    return df_allSimilars\n",
    "\n",
    "df_allSimilars = get_Statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for plagiarisms in documents: document05549 - document05998\n",
      "------------------------------------------------------------------------\n",
      "Looking for plagiarisms...\n",
      "Plagiarism Found in 5-Shingle:  ['\\ufeffProduced', 'by', 'David', 'Widger', 'QUOTES']\n",
      "Plagiarism Found in 5-Shingle:  ['by', 'David', 'Widger', 'QUOTES', 'AND']\n",
      "Plagiarism Found in 5-Shingle:  ['David', 'Widger', 'QUOTES', 'AND', 'IMAGES']\n",
      "Plagiarism Found in 5-Shingle:  ['Widger', 'QUOTES', 'AND', 'IMAGES', 'MEMOIRS']\n",
      "Plagiarism Found in 5-Shingle:  ['QUOTES', 'AND', 'IMAGES', 'MEMOIRS', 'OF']\n",
      "Plagiarism Found in 5-Shingle:  ['If', 'you', 'wish', 'to', 'read']\n",
      "Plagiarism Found in 5-Shingle:  ['you', 'wish', 'to', 'read', 'the']\n",
      "Plagiarism Found in 5-Shingle:  ['wish', 'to', 'read', 'the', 'entire']\n",
      "Plagiarism Found in 5-Shingle:  ['to', 'read', 'the', 'entire', 'context']\n",
      "Plagiarism Found in 5-Shingle:  ['read', 'the', 'entire', 'context', 'of']\n",
      "Plagiarism Found in 5-Shingle:  ['the', 'entire', 'context', 'of', 'any']\n",
      "Plagiarism Found in 5-Shingle:  ['entire', 'context', 'of', 'any', 'of']\n",
      "Plagiarism Found in 5-Shingle:  ['context', 'of', 'any', 'of', 'these']\n",
      "Plagiarism Found in 5-Shingle:  ['of', 'any', 'of', 'these', 'quotations']\n",
      "Plagiarism Found in 5-Shingle:  ['any', 'of', 'these', 'quotations', 'select']\n",
      "Plagiarism Found in 5-Shingle:  ['of', 'these', 'quotations', 'select', 'a']\n",
      "Plagiarism Found in 5-Shingle:  ['these', 'quotations', 'select', 'a', 'short']\n",
      "Plagiarism Found in 5-Shingle:  ['quotations', 'select', 'a', 'short', 'segment']\n",
      "Plagiarism Found in 5-Shingle:  ['select', 'a', 'short', 'segment', 'and']\n",
      "Plagiarism Found in 5-Shingle:  ['a', 'short', 'segment', 'and', 'copy']\n",
      "Plagiarism Found in 5-Shingle:  ['short', 'segment', 'and', 'copy', 'it']\n",
      "Plagiarism Found in 5-Shingle:  ['segment', 'and', 'copy', 'it', 'into']\n",
      "Plagiarism Found in 5-Shingle:  ['and', 'copy', 'it', 'into', 'your']\n",
      "Plagiarism Found in 5-Shingle:  ['copy', 'it', 'into', 'your', 'clipboard']\n",
      "Plagiarism Found in 5-Shingle:  ['it', 'into', 'your', 'clipboard', 'memorythen']\n",
      "Plagiarism Found in 5-Shingle:  ['into', 'your', 'clipboard', 'memorythen', 'open']\n",
      "Plagiarism Found in 5-Shingle:  ['your', 'clipboard', 'memorythen', 'open', 'the']\n",
      "Plagiarism Found in 5-Shingle:  ['clipboard', 'memorythen', 'open', 'the', 'following']\n",
      "Plagiarism Found in 5-Shingle:  ['memorythen', 'open', 'the', 'following', 'eBook']\n",
      "Plagiarism Found in 5-Shingle:  ['open', 'the', 'following', 'eBook', 'and']\n",
      "Plagiarism Found in 5-Shingle:  ['the', 'following', 'eBook', 'and', 'paste']\n",
      "Plagiarism Found in 5-Shingle:  ['following', 'eBook', 'and', 'paste', 'the']\n",
      "Plagiarism Found in 5-Shingle:  ['eBook', 'and', 'paste', 'the', 'phrase']\n",
      "Plagiarism Found in 5-Shingle:  ['and', 'paste', 'the', 'phrase', 'into']\n",
      "Plagiarism Found in 5-Shingle:  ['paste', 'the', 'phrase', 'into', 'your']\n",
      "Plagiarism Found in 5-Shingle:  ['the', 'phrase', 'into', 'your', 'computers']\n",
      "Plagiarism Found in 5-Shingle:  ['phrase', 'into', 'your', 'computers', 'find']\n",
      "Plagiarism Found in 5-Shingle:  ['into', 'your', 'computers', 'find', 'or']\n",
      "Plagiarism Found in 5-Shingle:  ['your', 'computers', 'find', 'or', 'search']\n",
      "Plagiarism Found in 5-Shingle:  ['computers', 'find', 'or', 'search', 'operation']\n",
      "Plagiarism Found in 5-Shingle:  ['find', 'or', 'search', 'operation', 'Memoirs']\n",
      "Plagiarism Found in 5-Shingle:  ['or', 'search', 'operation', 'Memoirs', 'of']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Searcing for plagiarisms in documents: document02150 - document03415\n",
      "------------------------------------------------------------------------\n",
      "Looking for plagiarisms...\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "#                 *** DETECTING PLAGIARISMS IN CANDIDATE PAIRS ***\n",
    "#################################################################################\n",
    "\n",
    "class detect_Plagiarism():\n",
    "\n",
    "    def __init__(self, doc1, doc2):\n",
    "    \n",
    "        self.doc1_text = doc1\n",
    "        self.doc2_text = doc2\n",
    "        self.k = 5\n",
    "        self.synonyms = json.load(open('node_modules/synonyms/src.json', 'r'))\n",
    "        self.preprocess_Text()\n",
    "        self.detect_plagShingles()\n",
    "\n",
    "    def preprocess_Text(self):\n",
    "        \n",
    "        \n",
    "        replace = str.maketrans('', '', string.punctuation)\n",
    "        \n",
    "        # Removing punctuation from document 1\n",
    "        self.doc1_words = self.doc1_text.translate(replace)\n",
    "        # This is a list which keeps all words of document 1\n",
    "        self.doc1_words = self.doc1_words.split()\n",
    "     \n",
    "\n",
    "        # Removing punctuation document 2\n",
    "        self.mlist = self.doc2_text.translate(replace)\n",
    "        # This is a list which keeps all words of document 2\n",
    "        self.mlist = self.mlist.split()\n",
    "      \n",
    "        \n",
    "        \n",
    "    def find_Synonyms(self):\n",
    "        self.mutator_list = []\n",
    "        for pos, word in enumerate(self.aShingle):\n",
    "            \n",
    "            if word in self.synonyms.keys():\n",
    "                \n",
    "                syns_list = []\n",
    "\n",
    "                for key, item in self.synonyms[word].items():\n",
    "                    syns_list += item\n",
    "\n",
    "                self.mutator_list.append(list(set(syns_list)))\n",
    "                    \n",
    "            else:\n",
    "                self.mutator_list.append([word])\n",
    "            \n",
    "        self.master_list = list(itertools.product(*self.mutator_list))\n",
    "        \n",
    "        for aList in self.master_list:\n",
    "            result = any(list(aList) == self.mlist[it:self.k +it] for it in range(len(self.mlist) -1))\n",
    "            if result:\n",
    "                print (\"Plagiarism Found in 5-Shingle: \", list(aList))\n",
    "               \n",
    "\n",
    "    def detect_plagShingles(self):\n",
    "       \n",
    "        print (\"Looking for plagiarisms...\")\n",
    "       \n",
    "        for i,j in enumerate(self.doc1_words):\n",
    "            self.master_list = []\n",
    "            self.aShingle = self.doc1_words[i:self.k+i]\n",
    "            \n",
    "            # If the length of shingle is less than k continue\n",
    "            if len(self.aShingle) < self.k:\n",
    "                continue\n",
    "            # If the shingle is not yet contained in the list of shingle\n",
    "            if self.aShingle not in self.master_list:\n",
    "                self.master_list.append(self.aShingle)\n",
    "            \n",
    "            # Check for synonyms\n",
    "            self.find_Synonyms()\n",
    "           \n",
    "\n",
    "\n",
    "for index, rows in df_allSimilars.iterrows(): \n",
    "    doc1 = str(srcdir) + \"/source-\" + rows.Document_1 + \".txt\"\n",
    "    doc2 = str(srcdir) + \"/source-\" + rows.Document_2 + \".txt\"\n",
    "    print(\"Searcing for plagiarisms in documents: \" + str(rows.Document_1) + \" - \" + str(rows.Document_2))\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    quick = detect_Plagiarism(open(doc1, 'r').read(), open(doc2, 'r').read())\n",
    "    print(\"\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  I didn't let the process to run till the end, in order to not print too many plagiarisms that will not help us check the results. \n",
    "### Of course you can run the code on your own at any time to check it farther."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
